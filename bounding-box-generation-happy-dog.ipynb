{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11240443,"sourceType":"datasetVersion","datasetId":7022652}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n    # for filename in filenames:\n    #     print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T19:56:10.750751Z","iopub.execute_input":"2025-04-02T19:56:10.750986Z","iopub.status.idle":"2025-04-02T19:56:11.622763Z","shell.execute_reply.started":"2025-04-02T19:56:10.750964Z","shell.execute_reply":"2025-04-02T19:56:11.622031Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nimport random\nfrom sklearn.model_selection import train_test_split\n\nclass DogFaceDataset(Dataset):\n    def __init__(self, images, annotations, transform=None):\n        \"\"\"\n        Args:\n            images (list): List of image file paths.\n            annotations (list): List of annotation file paths.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.images = images\n        self.annotations = annotations\n        self.transform = transform\n        \n    def parse_xml(self, xml_file):\n        \"\"\"Parse XML annotation file to extract bounding box information.\"\"\"\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        boxes = []\n        \n        for obj in root.findall('object'):\n            bbox = obj.find('bndbox')\n            xmin = int(bbox.find('xmin').text)\n            ymin = int(bbox.find('ymin').text)\n            xmax = int(bbox.find('xmax').text)\n            ymax = int(bbox.find('ymax').text)\n            boxes.append([xmin, ymin, xmax, ymax])\n            break\n        \n        return boxes if boxes else None  # Return None if no bounding box is found\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        annotation_path = self.annotations[idx]\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        boxes = self.parse_xml(annotation_path)\n        \n        target = {}\n        if boxes:\n            target['boxes'] = torch.tensor(boxes, dtype=torch.float32)\n            target['labels'] = torch.tensor([1] * len(boxes), dtype=torch.int64)  # Class 1 for dog\n\n        if not boxes:\n            target['boxes'] = torch.empty(0, 4)  # No boxes\n            target['labels'] = torch.empty((0,), dtype=torch.int64)\n            \n        if self.transform:\n            image = self.transform(image)\n            image = image.squeeze(1)\n        \n        return image, target\n\n\ndef split_dataset(image_paths, annotation_paths, val_size=0.1, test_size=0.1):\n    \"\"\"\n    Split dataset into train, validation, and test sets.\n    \n    Args:\n        image_paths (list): List of image file paths.\n        annotation_paths (list): List of annotation file paths.\n        val_size (float): Proportion of data to be used for validation.\n        test_size (float): Proportion of data to be used for testing.\n        \n    Returns:\n        Tuple of lists containing image and annotation file paths for train, val, and test sets.\n    \"\"\"\n    # Split into train + temp (val + test)\n    train_imgs, temp_imgs, train_ann, temp_ann = train_test_split(image_paths, annotation_paths, test_size=val_size + test_size, random_state=42)\n    \n    # Split temp into val and test\n    val_imgs, test_imgs, val_ann, test_ann = train_test_split(temp_imgs, temp_ann, test_size=test_size / (val_size + test_size), random_state=42)\n    \n    return train_imgs, train_ann, val_imgs, val_ann, test_imgs, test_ann\n\ndef get_loaders(data_dir, batch_size=16, num_workers=4):\n    image_dir = os.path.join(data_dir, \"Images\")\n    annotation_dir = os.path.join(data_dir, \"Annotations\")\n    \n    # Get the list of image and annotation file paths\n    image_paths = []\n    annotation_paths = []\n    \n    for category in os.listdir(image_dir):\n        category_path = os.path.join(image_dir, category)\n        annotation_path = os.path.join(annotation_dir, category)\n        \n        if os.path.isdir(category_path):\n            for img_name in os.listdir(category_path):\n                img_path = os.path.join(category_path, img_name)\n                annotation_file = os.path.join(annotation_path, img_name.replace('.jpg', ''))\n                \n                if os.path.exists(annotation_file):\n                    image_paths.append(img_path)\n                    annotation_paths.append(annotation_file)\n    \n    # Split the dataset into train, validation, and test sets\n    train_imgs, train_ann, val_imgs, val_ann, test_imgs, test_ann = split_dataset(image_paths, annotation_paths)\n\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ])\n\n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n\n    # Create datasets for each split\n    train_dataset = DogFaceDataset(train_imgs, train_ann, transform=train_transform)\n    val_dataset = DogFaceDataset(val_imgs, val_ann, transform=test_transform)\n    test_dataset = DogFaceDataset(test_imgs, test_ann, transform=test_transform)\n\n    # Create DataLoaders for each split\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:12:56.995686Z","iopub.execute_input":"2025-04-02T20:12:56.996010Z","iopub.status.idle":"2025-04-02T20:12:57.009215Z","shell.execute_reply.started":"2025-04-02T20:12:56.995986Z","shell.execute_reply":"2025-04-02T20:12:57.008180Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_loaders(\"/kaggle/input/imagenet/bounding_box_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:13:01.062097Z","iopub.execute_input":"2025-04-02T20:13:01.062387Z","iopub.status.idle":"2025-04-02T20:13:11.513686Z","shell.execute_reply.started":"2025-04-02T20:13:01.062365Z","shell.execute_reply":"2025-04-02T20:13:11.512964Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms\nfrom torchvision.models.detection import FasterRCNN, fasterrcnn_resnet50_fpn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:07:10.197832Z","iopub.execute_input":"2025-04-02T20:07:10.198130Z","iopub.status.idle":"2025-04-02T20:07:10.202274Z","shell.execute_reply.started":"2025-04-02T20:07:10.198089Z","shell.execute_reply":"2025-04-02T20:07:10.201505Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Load a pre-trained Faster R-CNN model (ResNet-50 backbone)\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Modify the classifier (in this case, we only need 2 classes: background and dog)\nnum_classes = 2  # 1 class (dog) + background\n\n# Get the input features from the pre-trained model\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# Replace the predictor with a new one for our custom dataset\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:07:10.203914Z","iopub.execute_input":"2025-04-02T20:07:10.204232Z","iopub.status.idle":"2025-04-02T20:07:10.947035Z","shell.execute_reply.started":"2025-04-02T20:07:10.204207Z","shell.execute_reply":"2025-04-02T20:07:10.946117Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(deevice)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:00:06.277250Z","iopub.execute_input":"2025-04-02T20:00:06.277541Z","iopub.status.idle":"2025-04-02T20:00:06.612093Z","shell.execute_reply.started":"2025-04-02T20:00:06.277520Z","shell.execute_reply":"2025-04-02T20:00:06.611346Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:00:21.456474Z","iopub.execute_input":"2025-04-02T20:00:21.456792Z","iopub.status.idle":"2025-04-02T20:00:21.461919Z","shell.execute_reply.started":"2025-04-02T20:00:21.456771Z","shell.execute_reply":"2025-04-02T20:00:21.460992Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:01:30.151576Z","iopub.execute_input":"2025-04-02T20:01:30.151915Z","iopub.status.idle":"2025-04-02T20:01:30.156584Z","shell.execute_reply.started":"2025-04-02T20:01:30.151891Z","shell.execute_reply":"2025-04-02T20:01:30.155699Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\nlr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:01:32.263619Z","iopub.execute_input":"2025-04-02T20:01:32.263940Z","iopub.status.idle":"2025-04-02T20:01:32.267930Z","shell.execute_reply.started":"2025-04-02T20:01:32.263916Z","shell.execute_reply":"2025-04-02T20:01:32.267057Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    # Iterate over the training data\n    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        if len(images) == 0 or len(targets) == 0:\n            continue  # Skip this batch\n\n        # # Move images and targets to the correct device (GPU or CPU)\n        # images = [image.squeeze(1).to(device) for image in images]\n        targets = [{k: v.to(device) for k, v in targets.items()}]\n        images = images.to(device)\n        print(targets)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass: Get predictions\n        loss_dict = model(images, targets)\n        \n        # Get total loss (sum of all losses)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Backward pass: Compute gradients\n        losses.backward()\n        \n        # Update the weights\n        optimizer.step()\n        \n        # Track running loss\n        running_loss += losses.item()\n    \n    # Print the loss for the current epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n\n    # Step the learning rate scheduler (optional)\n    lr_scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:14:20.080958Z","iopub.execute_input":"2025-04-02T20:14:20.081273Z","iopub.status.idle":"2025-04-02T20:14:21.170920Z","shell.execute_reply.started":"2025-04-02T20:14:20.081250Z","shell.execute_reply":"2025-04-02T20:14:21.169738Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10:   0%|          | 0/1029 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[{'boxes': tensor([[[ 25.,  48., 373., 498.]],\n\n        [[ 27., 155., 296., 416.]],\n\n        [[ 40.,  32., 459., 455.]],\n\n        [[114.,  11., 345., 350.]],\n\n        [[  0.,  33., 471., 373.]],\n\n        [[ 22.,  53., 263., 430.]],\n\n        [[  1.,  17., 404., 498.]],\n\n        [[291., 129., 477., 271.]],\n\n        [[168.,  97., 383., 265.]],\n\n        [[  0.,  10., 282., 231.]],\n\n        [[ 22.,   6., 499., 341.]],\n\n        [[203., 146., 384., 305.]],\n\n        [[ 33.,  30., 336., 498.]],\n\n        [[ 32., 150., 234., 283.]],\n\n        [[225., 118., 299., 334.]],\n\n        [[  1.,  56., 340., 498.]]], device='cuda:0'), 'labels': tensor([[1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1],\n        [1]], device='cuda:0')}]\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 0/1029 [00:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-fcb339eed17d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Forward pass: Get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Get total loss (sum of all losses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                         torch._assert(\n\u001b[0m\u001b[1;32m     68\u001b[0m                             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                             \u001b[0;34mf\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   2038\u001b[0m             \u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m         )\n\u001b[0;32m-> 2040\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([16, 1, 4])."],"ename":"AssertionError","evalue":"Expected target boxes to be a tensor of shape [N, 4], got torch.Size([16, 1, 4]).","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, _ = next(iter(train_loader))\n\nmodel()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}